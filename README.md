## Daniel Dimick

AI Engineer and Enterprise Transformation Consultant bridging strategy, transformation, and technical implementation.

**What I build:** Fine-tuned SLMs, multi-agent architectures, RAG systems, and agentic workflows for enterprise AI.

**Case Study:** [Enterprise AI Transformation](https://deepdmk.github.io/enterprise-ai-case-study/) - From McKinsey 7S through 14 fine-tuned models and a learned orchestrator. $163K total investment vs. $2-7M vendor approaches.

**Projects:**

| Repository | Focus |
|------------|-------|
| [enterprise-ai-case-study](https://github.com/deepdmk/enterprise-ai-case-study) | Full enterprise AI transformation architecture: 6-phase technical implementation with production-ready code. |
| [multi-agent-orchestration-beeai](https://github.com/deepdmk/multi-agent-orchestration-beeai) | 12-stage progression through agentic AI: tool integration, execution control, and multi-agent coordination. |
| [video-transcript-rag-system](https://github.com/deepdmk/video-transcript-rag-system) | RAG system with transcript extraction, FAISS semantic search, and Gradio interface powered by LangChain. |
| [lora-instruction-finetuning](https://github.com/deepdmk/lora-instruction-finetuning) | Parameter-efficient fine-tuning using LoRA. Trains <1% of parameters with SACREBLEU evaluation. |
| [dpo-preference-alignment](https://github.com/deepdmk/dpo-preference-alignment) | Direct Preference Optimization with LoRA adapters and 4-bit quantization. |
| [transformer-translation-from-scratch](https://github.com/deepdmk/transformer-translation-from-scratch) | Full encoder-decoder transformer built from scratch in PyTorch. |

**Certifications:** IBM AI Engineering Professional | IBM Advanced RAG & Agentic AI Professional

**Connect:** [LinkedIn](https://www.linkedin.com/in/dpdimick/) | [Medium](https://medium.com/@thedevelopingedge)
